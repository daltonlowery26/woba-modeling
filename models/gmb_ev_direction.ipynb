{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66563c33",
   "metadata": {},
   "source": [
    "### lgb to model wobacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80e416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, PredefinedSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_pinball_loss, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b48f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/dalto/OneDrive/Pictures/Documents/Projects/Coding Projects/woba modeling/data/')\n",
    "df = pd.read_csv('pitch/pitch_cleaned.csv').drop(columns=['Unnamed: 0'])\n",
    "df['year'] = pd.to_datetime(df['year']).dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16116f",
   "metadata": {},
   "source": [
    "##### Cleaning for Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abeb93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['description'] == 'hit_into_play']\n",
    "df = df[['batter','year', 'woba_value', 'launch_speed', 'launch_angle', 'spray_angle']]\n",
    "df = df[df['launch_speed'].notna()] # mcar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d897b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd80137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df[df['year'] < 2025]\n",
    "df_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "748ae828",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (df_train[['launch_speed', 'launch_angle', 'spray_angle']])\n",
    "y = df_train['woba_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37995345",
   "metadata": {},
   "source": [
    "##### Train Val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2cc10",
   "metadata": {},
   "source": [
    "no need for a test set as I am purposely holding out 2025 data. I want to test on all 2025 data to compare the predection power of this model to xwobacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588c9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=26) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bc7ff",
   "metadata": {},
   "source": [
    "##### Hyper Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50c75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(random_state=26, n_jobs=5, metric='quantile', objective='quantile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450d758",
   "metadata": {},
   "source": [
    "##### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7800b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0904ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Randomized Search for quantile: 0.05\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.05, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters for quantile 0.05: {'subsample': 0.55, 'num_leaves': 200, 'n_estimators': 600, 'min_data_in_leaf': 35, 'max_depth': 19, 'learning_rate': 0.1, 'lambda_l2': 50, 'lambda_l1': 5, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.05: 0.018310681363216962\n",
      "Running Randomized Search for quantile: 0.15\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.15, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.15: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.15: 0.04879263976510909\n",
      "Running Randomized Search for quantile: 0.25\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.25, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.25: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.25: 0.07013133763372575\n",
      "Running Randomized Search for quantile: 0.35\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.35, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.35: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.35: 0.08767339438206488\n",
      "Running Randomized Search for quantile: 0.45\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.45, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.45: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.45: 0.09974173328361766\n",
      "Running Randomized Search for quantile: 0.55\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.55, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.55: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.55: 0.1048596524893517\n",
      "Running Randomized Search for quantile: 0.65\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.65, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "Best parameters for quantile 0.65: {'subsample': 0.4666666666666667, 'num_leaves': 191, 'n_estimators': 700, 'min_data_in_leaf': 5, 'max_depth': 19, 'learning_rate': 0.1, 'lambda_l2': 1, 'lambda_l1': 3, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.65: 0.10454496965415179\n",
      "Running Randomized Search for quantile: 0.75\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.75, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.900000\n",
      "Best parameters for quantile 0.75: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.75: 0.0977717105041431\n",
      "Running Randomized Search for quantile: 0.85\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.85, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.900000\n",
      "Best parameters for quantile 0.85: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.85: 0.08068820142564045\n",
      "Running Randomized Search for quantile: 0.95\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.95, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 1.600000\n",
      "Best parameters for quantile 0.95: {'subsample': 0.6333333333333333, 'num_leaves': 175, 'n_estimators': 1100, 'min_data_in_leaf': 31, 'max_depth': 10, 'learning_rate': 0.1, 'lambda_l2': 50, 'lambda_l1': 1, 'colsample_bytree': 0.9, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.95: 0.03264365157806228\n"
     ]
    }
   ],
   "source": [
    "rnd_search_params = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.1, 0.01],          \n",
    "    'num_leaves': np.linspace(2, 200, 25, dtype=int),\n",
    "    'max_depth': np.linspace(2, 19, 7, dtype=int),  \n",
    "    'min_data_in_leaf': np.linspace(1, 40, 10, dtype=int),         \n",
    "    'subsample': np.linspace(0.3, 0.8, 7),               \n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),\n",
    "    'n_estimators': np.linspace(100, 1500, 15, dtype=int),\n",
    "    'lambda_l2': [1, 3, 5, 10, 20, 25, 50],\n",
    "    'lambda_l1': [0.001, 0.01, 1, 3, 5]\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\": [lgb.early_stopping(stopping_rounds=75, verbose=False)], \n",
    "    \"eval_set\": [(x_val, y_val)],\n",
    "    \"eval_metric\": 'quantile' \n",
    "}\n",
    "\n",
    "# for early stopping\n",
    "x_combined = np.concatenate((x_train, x_val), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
    "split_index = [-1] * len(x_train) + [0] * len(x_val)\n",
    "pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "all_best_params = {}\n",
    "all_best_scores = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    print(f\"Running Randomized Search for quantile: {q}\")\n",
    "\n",
    "    pinball_scorer = make_scorer(mean_pinball_loss, greater_is_better=False, alpha=q)\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "                            alpha=q,\n",
    "                            max_bin=63,\n",
    "                            random_state=26,\n",
    "                            n_jobs=5,\n",
    "                            metric='quantile', \n",
    "                            objective='quantile')\n",
    "\n",
    "    \n",
    "    rnd_searcher = RandomizedSearchCV(model, param_distributions=rnd_search_params, cv=pds, scoring=pinball_scorer,\n",
    "                                    n_iter=100, random_state=26, verbose=1, n_jobs=3) \n",
    "    \n",
    "    print(model.get_params)\n",
    "    search = rnd_searcher.fit(x_combined, y_combined, **fit_params)\n",
    "    \n",
    "    print(f\"Best parameters for quantile {q}: {search.best_params_}\")\n",
    "    print(f\"Best score for quantile {q}: {-search.best_score_}\")\n",
    "    \n",
    "    all_best_params[q] = search.best_params_\n",
    "    all_best_scores[q] = search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac5100",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bc454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'boosting_type': ['dart', 'gbdt'],\n",
    "        'subsample': [0.45, 0.5, 0.55], \n",
    "        'num_leaves':  [8, 9, 10], \n",
    "        'n_estimators': [350, 400, 450], \n",
    "        'min_data_in_leaf': [16, 18, 20], \n",
    "        'max_depth': [8, 9], \n",
    "        'max_bin': [63], \n",
    "        'learning_rate': [0.1, 0.15], \n",
    "        'lambda_l2': [0.5, 1], \n",
    "        'colsample_bytree': [0.75, 0.8]\n",
    "        }\n",
    "\n",
    "\n",
    "# for early stopping\n",
    "x_combined = np.concatenate((x_train, x_val), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
    "split_index = [-1] * len(x_train) + [0] * len(x_val)\n",
    "pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\": [lgb.early_stopping(stopping_rounds=40, verbose=False)], \n",
    "    \"eval_set\": [(x_val, y_val)],\n",
    "    \"eval_metric\": \"rmse\" \n",
    "}\n",
    "\n",
    "\n",
    "grid_searcher = GridSearchCV(model, param_grid=grid, cv=pds, verbose=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aaf053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 2592 candidates, totalling 2592 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 266\n",
      "[LightGBM] [Info] Number of data points in the train set: 169715, number of used features: 5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.373572\n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.75, 'lambda_l2': 1, 'learning_rate': 0.15, 'max_bin': 63, 'max_depth': 8, 'min_data_in_leaf': 18, 'n_estimators': 350, 'num_leaves': 9, 'subsample': 0.45}\n",
      "0.22591349039763697\n",
      "['Column_0' 'Column_1' 'Column_2' 'Column_3' 'Column_4']\n"
     ]
    }
   ],
   "source": [
    "grid_searcher.fit(x_combined, y_combined, **fit_params)\n",
    "print(grid_searcher.best_params_)\n",
    "print(grid_searcher.best_score_)\n",
    "print(grid_searcher.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4afe9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best = {'objective':'regression', 'type': 'quantilie', 'boosting_type': 'dart', 'colsample_bytree': 0.75, 'lambda_l2': 1, 'learning_rate': 0.15, 'max_bin': 63, 'max_depth': 8, 'min_data_in_leaf': 18, 'n_estimators': 350, 'num_leaves': 9, 'subsample': 0.45}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa024a79",
   "metadata": {},
   "source": [
    "#### Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c215ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best param\n"
     ]
    }
   ],
   "source": [
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [convert_numpy_types(i) for i in obj]\n",
    "    return obj\n",
    "\n",
    "serializable_params = convert_numpy_types(all_best_params)\n",
    "\n",
    "with open('ev_dir_params.json', 'w') as f:\n",
    "    json.dump(serializable_params, f, indent=4)\n",
    "\n",
    "print(\"Saved best param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c623eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:/Users/dalto/OneDrive/Pictures/Documents/Projects/Coding Projects/woba modeling/data/parameters/ev_dir_params.json', 'r') as f:\n",
    "    ev_dir_params = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f778a",
   "metadata": {},
   "source": [
    "#### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f8995a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 780192, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384751\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "quantiles = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]\n",
    "for q in quantiles:\n",
    "    quantile_model = lgb.LGBMRegressor(**ev_dir_params[str(q)], alpha=q, random_state=26, n_jobs=-1)\n",
    "    quantile_model.fit(x_train, y_train, \n",
    "                       eval_set=[(x_val, y_val)], \n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=40, verbose=False)])\n",
    "    models[q] = quantile_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f1543",
   "metadata": {},
   "source": [
    "##### Testing on 2025 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d221207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_25 = df[df['year'] == 2025]\n",
    "x_25 = df_25[['launch_speed', 'launch_angle', 'spray_angle']]\n",
    "y_25 = df_25['woba_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f355e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "RMSE: 0.37883292333874125\n"
     ]
    }
   ],
   "source": [
    "y_pred = models[.45].predict(x_25)\n",
    "rmse = np.sqrt(mean_squared_error(y_25, y_pred))\n",
    "results_df = pd.DataFrame({'actual': y_25, 'predicted': y_pred})\n",
    "results_df = results_df.join(df[['batter', 'year']])\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0e96e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb.plot_importance(models[.55], importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9753356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for batters with more than 60 plate appearances: 0.04463263716929888\n",
      "Grouped RMSE 0.11175876568018256\n"
     ]
    }
   ],
   "source": [
    "grouped_results = results_df.groupby(['year', 'batter'])[['actual', 'predicted']].agg(['mean', 'count'])\n",
    "grouped_results.columns = ['_'.join(col).strip() for col in grouped_results.columns.values]\n",
    "grouped_results = grouped_results.reset_index()\n",
    "grouped_rmse = np.sqrt(mean_squared_error(grouped_results['actual_mean'], grouped_results['predicted_mean']))\n",
    "qualified_results = grouped_results[grouped_results['actual_count'] > 60]\n",
    "qualified_rmse = np.sqrt(mean_squared_error(qualified_results['actual_mean'], qualified_results['predicted_mean']))\n",
    "print(f'RMSE for batters with more than 60 plate appearances: {qualified_rmse}')\n",
    "print(f'Grouped RMSE {grouped_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4f868",
   "metadata": {},
   "source": [
    "##### Find Quintiles for Each Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4859dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m quantile_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m:\n\u001b[0;32m      4\u001b[0m     quantile_predictions[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m models[q]\u001b[38;5;241m.\u001b[39mpredict(x_25)\n\u001b[0;32m      6\u001b[0m quantile_predictions\u001b[38;5;241m.\u001b[39mset_index(x_25\u001b[38;5;241m.\u001b[39mindex, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "quantile_predictions = pd.DataFrame()\n",
    "\n",
    "for q in models:\n",
    "    quantile_predictions[f'q_{q}'] = models[q].predict(x_25)\n",
    "\n",
    "quantile_predictions.set_index(x_25.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdeccc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_predictions['name'] = df_25['batter']\n",
    "quantile_predictions['year'] = df_25['year']\n",
    "quantile_cols = sorted([col for col in quantile_predictions.columns if col.startswith('q_')])\n",
    "quantile_predictions[quantile_cols] = np.sort(quantile_predictions[quantile_cols].values, axis=1)\n",
    "quantile_predictions[quantile_cols] = quantile_predictions[quantile_cols].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80a8063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 players with highest prediction standard deviation:\n",
      "name\n",
      "aaron judge      0.012219\n",
      "adael amador     0.012219\n",
      "kyren paris      0.011666\n",
      "shohei ohtani    0.011569\n",
      "luis torrens     0.011422\n",
      "Name: pred_std, dtype: float64\n",
      "Top 5 players with lowest prediction standard deviation:\n",
      "name\n",
      "jonah bride       0.006798\n",
      "tim anderson      0.006936\n",
      "brooks baldwin    0.007335\n",
      "gary sanchez      0.007544\n",
      "trey sweeney      0.007628\n",
      "Name: pred_std, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "player_quant = quantile_predictions.groupby('name').mean()\n",
    "player_quant['pitch_count'] = quantile_predictions.groupby('name').size()\n",
    "player_quant['pred_std'] = player_quant[quantile_cols].std(axis=1)\n",
    "player_quant = player_quant[player_quant['pitch_count'] > 60]\n",
    "\n",
    "\n",
    "# Find and print the top 5 players with the highest standard deviation\n",
    "print(\"Top 5 players with highest prediction standard deviation:\")\n",
    "top_5_highest_std = player_quant.nlargest(5, 'pred_std')\n",
    "print(top_5_highest_std['pred_std'])\n",
    "\n",
    "# Find and print the top 5 players with the lowest standard deviation\n",
    "print(\"Top 5 players with lowest prediction standard deviation:\")\n",
    "top_5_lowest_std = player_quant.nsmallest(5, 'pred_std')\n",
    "print(top_5_lowest_std['pred_std'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e20053",
   "metadata": {},
   "source": [
    "##### Quant Predections For Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "920d4f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for q in quantiles:\n",
    "    quantile_model = lgb.LGBMRegressor(**ev_dir_params[str(q)], alpha=q, random_state=26, n_jobs=-1)\n",
    "    quantile_model.fit(X, y, \n",
    "                       eval_set=[(x_val, y_val)], \n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=40, verbose=False)])\n",
    "    models[q] = quantile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9413a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n"
     ]
    }
   ],
   "source": [
    "full_predictions = pd.DataFrame()\n",
    "\n",
    "for q in models:\n",
    "    full_predictions[f'q_{q}'] = models[q].predict(X)\n",
    "\n",
    "full_predictions.set_index(X.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d419b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predictions['name'] = df['batter']\n",
    "full_predictions['year'] = df['year']\n",
    "quantile_cols = sorted([col for col in full_predictions.columns if col.startswith('q_')])\n",
    "full_predictions[quantile_cols] = np.sort(full_predictions[quantile_cols].values, axis=1)\n",
    "full_predictions[quantile_cols] = full_predictions[quantile_cols].clip(lower=0)\n",
    "full_predictions[quantile_cols] = full_predictions[quantile_cols].clip(upper=2.01775) # average hr woba over last 8 years\n",
    "full_predictions = full_predictions.reset_index()\n",
    "cols = ['name', 'year'] + [col for col in full_predictions.columns if col not in ['name', 'year', 'index']]\n",
    "full_predictions = full_predictions[cols]\n",
    "full_predictions.to_csv('quantile_predections/ev_dir_pitch.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
