{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66563c33",
   "metadata": {},
   "source": [
    "### lgb to model wobacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80e416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, PredefinedSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_pinball_loss, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b48f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/dalto/OneDrive/Pictures/Documents/Projects/Coding Projects/woba modeling/data/')\n",
    "df = pd.read_csv('pitch/pitch_cleaned.csv').drop(columns=['Unnamed: 0'])\n",
    "df['year'] = pd.to_datetime(df['year']).dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16116f",
   "metadata": {},
   "source": [
    "##### Cleaning for Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abeb93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['description'] == 'hit_into_play']\n",
    "df = df[['batter','year', 'woba_value', 'launch_speed', 'launch_angle', 'spray_angle']]\n",
    "df = df[df['launch_speed'].notna()] # mcar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d897b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd80137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df[df['year'] < 2025]\n",
    "df_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748ae828",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (df_train[['launch_speed', 'launch_angle']])\n",
    "y = df_train['woba_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37995345",
   "metadata": {},
   "source": [
    "##### Train Val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2cc10",
   "metadata": {},
   "source": [
    "no need for a test set as I am purposely holding out 2025 data. I want to test on all 2025 data to compare the predection power of this model to xwobacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588c9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=26) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bc7ff",
   "metadata": {},
   "source": [
    "##### Hyper Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50c75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(random_state=26, n_jobs=5, metric='quantile', objective='quantile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450d758",
   "metadata": {},
   "source": [
    "##### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7800b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0904ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Randomized Search for quantile: 0.05\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.05, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.05: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.05: 0.018554716287117707\n",
      "Running Randomized Search for quantile: 0.15\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.15, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.15: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.15: 0.05041433389072244\n",
      "Running Randomized Search for quantile: 0.25\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.25, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.25: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.25: 0.07682436026447635\n",
      "Running Randomized Search for quantile: 0.35\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.35, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.35: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.35: 0.0990597958084277\n",
      "Running Randomized Search for quantile: 0.45\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.45, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.45: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.45: 0.1142471761884223\n",
      "Running Randomized Search for quantile: 0.55\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.55, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.55: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.55: 0.12178841716585787\n",
      "Running Randomized Search for quantile: 0.65\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.65, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.65: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.65: 0.12075024685669425\n",
      "Running Randomized Search for quantile: 0.75\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.75, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.900000\n",
      "Best parameters for quantile 0.75: {'subsample': 0.4666666666666667, 'num_leaves': 191, 'n_estimators': 700, 'min_data_in_leaf': 5, 'max_depth': 19, 'learning_rate': 0.1, 'lambda_l2': 1, 'lambda_l1': 3, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.75: 0.11059857207227872\n",
      "Running Randomized Search for quantile: 0.85\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.85, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.900000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters for quantile 0.85: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.85: 0.08417874645370003\n",
      "Running Randomized Search for quantile: 0.95\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.95, max_bin=63, metric='quantile', n_jobs=5,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 875359, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 1.600000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters for quantile 0.95: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.95: 0.038645074355472764\n"
     ]
    }
   ],
   "source": [
    "rnd_search_params = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.1, 0.01],          \n",
    "    'num_leaves': np.linspace(2, 200, 25, dtype=int),\n",
    "    'max_depth': np.linspace(2, 19, 7, dtype=int),  \n",
    "    'min_data_in_leaf': np.linspace(1, 40, 10, dtype=int),         \n",
    "    'subsample': np.linspace(0.3, 0.8, 7),               \n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),\n",
    "    'n_estimators': np.linspace(100, 1500, 15, dtype=int),\n",
    "    'lambda_l2': [1, 3, 5, 10, 20, 25, 50],\n",
    "    'lambda_l1': [0.001, 0.01, 1, 3, 5]\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\": [lgb.early_stopping(stopping_rounds=75, verbose=False)], \n",
    "    \"eval_set\": [(x_val, y_val)],\n",
    "    \"eval_metric\": 'quantile' \n",
    "}\n",
    "\n",
    "# for early stopping\n",
    "x_combined = np.concatenate((x_train, x_val), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
    "split_index = [-1] * len(x_train) + [0] * len(x_val)\n",
    "pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "all_best_params = {}\n",
    "all_best_scores = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    print(f\"Running Randomized Search for quantile: {q}\")\n",
    "\n",
    "    pinball_scorer = make_scorer(mean_pinball_loss, greater_is_better=False, alpha=q)\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "                            alpha=q,\n",
    "                            max_bin=63,\n",
    "                            random_state=26,\n",
    "                            n_jobs=5,\n",
    "                            metric='quantile', \n",
    "                            objective='quantile')\n",
    "\n",
    "    \n",
    "    rnd_searcher = RandomizedSearchCV(model, param_distributions=rnd_search_params, cv=pds, scoring=pinball_scorer,\n",
    "                                    n_iter=60, random_state=26, verbose=1, n_jobs=3) \n",
    "    \n",
    "    print(model.get_params)\n",
    "    search = rnd_searcher.fit(x_combined, y_combined, **fit_params)\n",
    "    \n",
    "    print(f\"Best parameters for quantile {q}: {search.best_params_}\")\n",
    "    print(f\"Best score for quantile {q}: {-search.best_score_}\")\n",
    "    \n",
    "    all_best_params[q] = search.best_params_\n",
    "    all_best_scores[q] = search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa024a79",
   "metadata": {},
   "source": [
    "#### Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c215ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best param\n"
     ]
    }
   ],
   "source": [
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [convert_numpy_types(i) for i in obj]\n",
    "    return obj\n",
    "\n",
    "serializable_params = convert_numpy_types(all_best_params)\n",
    "\n",
    "with open('ev_dir_params.json', 'w') as f:\n",
    "    json.dump(serializable_params, f, indent=4)\n",
    "\n",
    "print(\"Saved best param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c623eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:/Users/dalto/OneDrive/Pictures/Documents/Projects/Coding Projects/woba modeling/data/parameters/ev_dir_params.json', 'r') as f:\n",
    "    ev_dir_params = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f778a",
   "metadata": {},
   "source": [
    "#### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f8995a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 787823, number of used features: 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "quantiles = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]\n",
    "for q in quantiles:\n",
    "    quantile_model = lgb.LGBMRegressor(**ev_dir_params[str(q)], alpha=q, random_state=26, n_jobs=-1)\n",
    "    quantile_model.fit(x_train, y_train, \n",
    "                       eval_set=[(x_val, y_val)], \n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=40, verbose=False)])\n",
    "    models[q] = quantile_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f1543",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f355e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "RMSE: 0.4309147881813464\n"
     ]
    }
   ],
   "source": [
    "y_pred = models[.55].predict(X)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "results_df = pd.DataFrame({'actual': y, 'predicted': y_pred})\n",
    "results_df = results_df.join(df[['batter', 'year']])\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ec44f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted': 0.4342526993517071}\n"
     ]
    }
   ],
   "source": [
    "stats = ['predicted']\n",
    "corr_next = results_df.groupby(['batter', 'year'])['predicted'].mean().reset_index()\n",
    "count = results_df.groupby(['batter', 'year']).size().reset_index(name='count')\n",
    "corr_next = corr_next.merge(count, on=['batter', 'year'])\n",
    "corr_next = corr_next[corr_next['count'] > 60]\n",
    "\n",
    "for stat in stats:\n",
    "    corr_next[f'{stat}_next'] = corr_next.groupby('batter')[stat].shift(-1)\n",
    "\n",
    "corr_next = corr_next.dropna(subset=[f'{stat}_next' for stat in stats])\n",
    "\n",
    "corrs = {}\n",
    "for stat in stats:\n",
    "    corrs[stat] = corr_next[[stat, f'{stat}_next']].corr().iloc[0, 1]\n",
    "    corrs[stat] = corrs[stat] ** 2\n",
    "\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e96e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb.plot_importance(models[.55], importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9753356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for batters with more than 60 plate appearances: 0.04463263716929888\n",
      "Grouped RMSE 0.11175876568018256\n"
     ]
    }
   ],
   "source": [
    "grouped_results = results_df.groupby(['year', 'batter'])[['actual', 'predicted']].agg(['mean', 'count'])\n",
    "grouped_results.columns = ['_'.join(col).strip() for col in grouped_results.columns.values]\n",
    "grouped_results = grouped_results.reset_index()\n",
    "grouped_rmse = np.sqrt(mean_squared_error(grouped_results['actual_mean'], grouped_results['predicted_mean']))\n",
    "qualified_results = grouped_results[grouped_results['actual_count'] > 60]\n",
    "qualified_rmse = np.sqrt(mean_squared_error(qualified_results['actual_mean'], qualified_results['predicted_mean']))\n",
    "print(f'RMSE for batters with more than 60 plate appearances: {qualified_rmse}')\n",
    "print(f'Grouped RMSE {grouped_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4f868",
   "metadata": {},
   "source": [
    "##### Find Quintiles for Each Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4859dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m quantile_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m:\n\u001b[0;32m      4\u001b[0m     quantile_predictions[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m models[q]\u001b[38;5;241m.\u001b[39mpredict(x_25)\n\u001b[0;32m      6\u001b[0m quantile_predictions\u001b[38;5;241m.\u001b[39mset_index(x_25\u001b[38;5;241m.\u001b[39mindex, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "quantile_predictions = pd.DataFrame()\n",
    "\n",
    "for q in models:\n",
    "    quantile_predictions[f'q_{q}'] = models[q].predict(x_25)\n",
    "\n",
    "quantile_predictions.set_index(x_25.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdeccc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_predictions['name'] = df_25['batter']\n",
    "quantile_predictions['year'] = df_25['year']\n",
    "quantile_cols = sorted([col for col in quantile_predictions.columns if col.startswith('q_')])\n",
    "quantile_predictions[quantile_cols] = np.sort(quantile_predictions[quantile_cols].values, axis=1)\n",
    "quantile_predictions[quantile_cols] = quantile_predictions[quantile_cols].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80a8063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 players with highest prediction standard deviation:\n",
      "name\n",
      "aaron judge      0.012219\n",
      "adael amador     0.012219\n",
      "kyren paris      0.011666\n",
      "shohei ohtani    0.011569\n",
      "luis torrens     0.011422\n",
      "Name: pred_std, dtype: float64\n",
      "Top 5 players with lowest prediction standard deviation:\n",
      "name\n",
      "jonah bride       0.006798\n",
      "tim anderson      0.006936\n",
      "brooks baldwin    0.007335\n",
      "gary sanchez      0.007544\n",
      "trey sweeney      0.007628\n",
      "Name: pred_std, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "player_quant = quantile_predictions.groupby('name').mean()\n",
    "player_quant['pitch_count'] = quantile_predictions.groupby('name').size()\n",
    "player_quant['pred_std'] = player_quant[quantile_cols].std(axis=1)\n",
    "player_quant = player_quant[player_quant['pitch_count'] > 60]\n",
    "\n",
    "\n",
    "# Find and print the top 5 players with the highest standard deviation\n",
    "print(\"Top 5 players with highest prediction standard deviation:\")\n",
    "top_5_highest_std = player_quant.nlargest(5, 'pred_std')\n",
    "print(top_5_highest_std['pred_std'])\n",
    "\n",
    "# Find and print the top 5 players with the lowest standard deviation\n",
    "print(\"Top 5 players with lowest prediction standard deviation:\")\n",
    "top_5_lowest_std = player_quant.nsmallest(5, 'pred_std')\n",
    "print(top_5_lowest_std['pred_std'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e20053",
   "metadata": {},
   "source": [
    "##### Quant Predections For Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "920d4f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for q in quantiles:\n",
    "    quantile_model = lgb.LGBMRegressor(**ev_dir_params[str(q)], alpha=q, random_state=26, n_jobs=-1)\n",
    "    quantile_model.fit(X, y, \n",
    "                       eval_set=[(x_val, y_val)], \n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=40, verbose=False)])\n",
    "    models[q] = quantile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9413a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n"
     ]
    }
   ],
   "source": [
    "full_predictions = pd.DataFrame()\n",
    "\n",
    "for q in models:\n",
    "    full_predictions[f'q_{q}'] = models[q].predict(X)\n",
    "\n",
    "full_predictions.set_index(X.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d419b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predictions['name'] = df['batter']\n",
    "full_predictions['year'] = df['year']\n",
    "quantile_cols = sorted([col for col in full_predictions.columns if col.startswith('q_')])\n",
    "full_predictions[quantile_cols] = np.sort(full_predictions[quantile_cols].values, axis=1)\n",
    "full_predictions[quantile_cols] = full_predictions[quantile_cols].clip(lower=0)\n",
    "full_predictions[quantile_cols] = full_predictions[quantile_cols].clip(upper=2.01775) # average hr woba over last 8 years\n",
    "full_predictions = full_predictions.reset_index()\n",
    "cols = ['name', 'year'] + [col for col in full_predictions.columns if col not in ['name', 'year', 'index']]\n",
    "full_predictions = full_predictions[cols]\n",
    "full_predictions.to_csv('quantile_predections/ev_dir_pitch.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
