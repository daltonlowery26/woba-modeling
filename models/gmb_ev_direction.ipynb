{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66563c33",
   "metadata": {},
   "source": [
    "### lgb to model wobacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80e416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, PredefinedSplit\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b48f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/dalto/OneDrive/Pictures/Documents/Projects/Coding Projects/woba modeling/data/')\n",
    "df = pd.read_csv('pitch/pitch_cleaned.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16116f",
   "metadata": {},
   "source": [
    "##### Cleaning for Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abeb93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['description'] == 'hit_into_play']\n",
    "df = df[['batter','year', 'woba_value', 'launch_speed', 'launch_angle', 'spray_angle']]\n",
    "df = df[df['launch_speed'].notna()] # mcar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d897b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd80137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df[df['year'] < 2025]\n",
    "df_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748ae828",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (df_train[['launch_speed', 'launch_angle', 'spray_angle']])\n",
    "y = df_train['woba_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37995345",
   "metadata": {},
   "source": [
    "##### Train Val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2cc10",
   "metadata": {},
   "source": [
    "no need for a test set as I am purposely holding out 2025 data. I want to test on all 2025 data to compare the predection power of this model to xwobacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588c9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=26) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bc7ff",
   "metadata": {},
   "source": [
    "##### Hyper Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50c75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(random_state=26, n_jobs=3, metric='quantile', objective='quantile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450d758",
   "metadata": {},
   "source": [
    "##### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7800b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d0904ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Randomized Search for quantile: 0.1\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.1, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.1: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.1: -0.019229608219393457\n",
      "Running Randomized Search for quantile: 0.2\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.2, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.2: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.2: 0.21494809036922913\n",
      "Running Randomized Search for quantile: 0.3\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.3, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.3: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.3: 0.33052674163092777\n",
      "Running Randomized Search for quantile: 0.4\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.4, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "Best parameters for quantile 0.4: {'subsample': 0.8, 'num_leaves': 150, 'n_estimators': 200, 'min_data_in_leaf': 22, 'max_depth': 19, 'learning_rate': 0.1, 'lambda_l2': 50, 'lambda_l1': 1, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.4: 0.3965347606396471\n",
      "Running Randomized Search for quantile: 0.5\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.5, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "Best parameters for quantile 0.5: {'subsample': 0.4666666666666667, 'num_leaves': 167, 'n_estimators': 1100, 'min_data_in_leaf': 9, 'max_depth': 13, 'learning_rate': 0.01, 'lambda_l2': 20, 'lambda_l1': 1, 'colsample_bytree': 0.9, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.5: 0.4362869732874797\n",
      "Running Randomized Search for quantile: 0.6\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.6, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "Best parameters for quantile 0.6: {'subsample': 0.7166666666666666, 'num_leaves': 191, 'n_estimators': 1500, 'min_data_in_leaf': 5, 'max_depth': 10, 'learning_rate': 0.01, 'lambda_l2': 25, 'lambda_l1': 1, 'colsample_bytree': 0.9, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.6: 0.4388580851892486\n",
      "Running Randomized Search for quantile: 0.7\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.7, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.900000\n",
      "Best parameters for quantile 0.7: {'subsample': 0.7166666666666666, 'num_leaves': 200, 'n_estimators': 200, 'min_data_in_leaf': 9, 'max_depth': 16, 'learning_rate': 0.1, 'lambda_l2': 20, 'lambda_l1': 5, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.7: 0.2837507294815774\n",
      "Running Randomized Search for quantile: 0.8\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.8, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.900000\n",
      "Best parameters for quantile 0.8: {'subsample': 0.4666666666666667, 'num_leaves': 191, 'n_estimators': 700, 'min_data_in_leaf': 5, 'max_depth': 19, 'learning_rate': 0.1, 'lambda_l2': 1, 'lambda_l1': 3, 'colsample_bytree': 1.0, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.8: 0.03325644230905078\n",
      "Running Randomized Search for quantile: 0.9\n",
      "<bound method LGBMModel.get_params of LGBMRegressor(alpha=0.9, max_bin=63, metric='quantile', n_jobs=3,\n",
      "              objective='quantile', random_state=26)>\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 189\n",
      "[LightGBM] [Info] Number of data points in the train set: 728384, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 1.250000\n",
      "Best parameters for quantile 0.9: {'subsample': 0.6333333333333333, 'num_leaves': 175, 'n_estimators': 1100, 'min_data_in_leaf': 31, 'max_depth': 10, 'learning_rate': 0.1, 'lambda_l2': 50, 'lambda_l1': 1, 'colsample_bytree': 0.9, 'boosting_type': 'gbdt'}\n",
      "Best score for quantile 0.9: -0.4632790277049854\n"
     ]
    }
   ],
   "source": [
    "rnd_search_params = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.1, 0.01],          \n",
    "    'num_leaves': np.linspace(2, 200, 25, dtype=int),\n",
    "    'max_depth': np.linspace(2, 19, 7, dtype=int),  \n",
    "    'min_data_in_leaf': np.linspace(1, 40, 10, dtype=int),         \n",
    "    'subsample': np.linspace(0.3, 0.8, 7),               \n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),\n",
    "    'n_estimators': np.linspace(100, 1500, 15, dtype=int),\n",
    "    'lambda_l2': [1, 3, 5, 10, 20, 25, 50],\n",
    "    'lambda_l1': [0.001, 0.01, 1, 3, 5]\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\": [lgb.early_stopping(stopping_rounds=40, verbose=False)], \n",
    "    \"eval_set\": [(x_val, y_val)],\n",
    "    \"eval_metric\": \"rmse\" \n",
    "}\n",
    "\n",
    "# for early stopping\n",
    "x_combined = np.concatenate((x_train, x_val), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
    "split_index = [-1] * len(x_train) + [0] * len(x_val)\n",
    "pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "all_best_params = {}\n",
    "all_best_scores = {}\n",
    "\n",
    "model.set_params(max_bin=63)\n",
    "for q in quantiles:\n",
    "    print(f\"Running Randomized Search for quantile: {q}\")\n",
    "    \n",
    "    rnd_searcher = RandomizedSearchCV(model, param_distributions=rnd_search_params, cv=pds,\n",
    "                                    n_iter=100, random_state=26, verbose=1, n_jobs=3) \n",
    "    \n",
    "    model.set_params(alpha=q)\n",
    "    print(model.get_params)\n",
    "    search = rnd_searcher.fit(x_combined, y_combined, **fit_params)\n",
    "    \n",
    "    print(f\"Best parameters for quantile {q}: {search.best_params_}\")\n",
    "    print(f\"Best score for quantile {q}: {search.best_score_}\")\n",
    "    \n",
    "    all_best_params[q] = search.best_params_\n",
    "    all_best_scores[q] = search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac5100",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bc454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'boosting_type': ['dart', 'gbdt'],\n",
    "        'subsample': [0.45, 0.5, 0.55], \n",
    "        'num_leaves':  [8, 9, 10], \n",
    "        'n_estimators': [350, 400, 450], \n",
    "        'min_data_in_leaf': [16, 18, 20], \n",
    "        'max_depth': [8, 9], \n",
    "        'max_bin': [63], \n",
    "        'learning_rate': [0.1, 0.15], \n",
    "        'lambda_l2': [0.5, 1], \n",
    "        'colsample_bytree': [0.75, 0.8]\n",
    "        }\n",
    "\n",
    "\n",
    "# for early stopping\n",
    "x_combined = np.concatenate((x_train, x_val), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
    "split_index = [-1] * len(x_train) + [0] * len(x_val)\n",
    "pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "fit_params = {\n",
    "    \"callbacks\": [lgb.early_stopping(stopping_rounds=40, verbose=False)], \n",
    "    \"eval_set\": [(x_val, y_val)],\n",
    "    \"eval_metric\": \"rmse\" \n",
    "}\n",
    "\n",
    "\n",
    "grid_searcher = GridSearchCV(model, param_grid=grid, cv=pds, verbose=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aaf053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 2592 candidates, totalling 2592 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 266\n",
      "[LightGBM] [Info] Number of data points in the train set: 169715, number of used features: 5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.373572\n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.75, 'lambda_l2': 1, 'learning_rate': 0.15, 'max_bin': 63, 'max_depth': 8, 'min_data_in_leaf': 18, 'n_estimators': 350, 'num_leaves': 9, 'subsample': 0.45}\n",
      "0.22591349039763697\n",
      "['Column_0' 'Column_1' 'Column_2' 'Column_3' 'Column_4']\n"
     ]
    }
   ],
   "source": [
    "grid_searcher.fit(x_combined, y_combined, **fit_params)\n",
    "print(grid_searcher.best_params_)\n",
    "print(grid_searcher.best_score_)\n",
    "print(grid_searcher.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4afe9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best = {'objective':'regression', 'type': 'quantilie', 'boosting_type': 'dart', 'colsample_bytree': 0.75, 'lambda_l2': 1, 'learning_rate': 0.15, 'max_bin': 63, 'max_depth': 8, 'min_data_in_leaf': 18, 'n_estimators': 350, 'num_leaves': 9, 'subsample': 0.45}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa024a79",
   "metadata": {},
   "source": [
    "#### Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best param\n"
     ]
    }
   ],
   "source": [
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [convert_numpy_types(i) for i in obj]\n",
    "    return obj\n",
    "\n",
    "serializable_params = convert_numpy_types(all_best_params)\n",
    "\n",
    "with open('ev_dir_params.json', 'w') as f:\n",
    "    json.dump(serializable_params, f, indent=4)\n",
    "\n",
    "print(\"Saved best param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c623eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:/Users/dalto/OneDrive/Pictures/Documents/Projects/Coding Projects/woba modeling/data/parameters/ev_dir.json', 'r') as f:\n",
    "    ev_dir_params = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f778a",
   "metadata": {},
   "source": [
    "#### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f8995a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 648341, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.385290\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for q in quantiles:\n",
    "    quantile_model = lgb.LGBMRegressor(**ev_dir_params[str(q)], alpha=q, random_state=26, n_jobs=-1)\n",
    "    quantile_model.fit(x_train, y_train, \n",
    "                       eval_set=[(x_val, y_val)], \n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=40, verbose=False)])\n",
    "    models[q] = quantile_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f1543",
   "metadata": {},
   "source": [
    "##### Testing on 2025 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d221207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_25 = df[df['year'] == 2025]\n",
    "x_25 = df_25[['launch_speed', 'launch_angle', 'spray_angle']]\n",
    "y_25 = df_25['woba_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f355e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "RMSE: 0.3850506591059303\n"
     ]
    }
   ],
   "source": [
    "y_pred = models[.5].predict(x_25)\n",
    "rmse = np.sqrt(mean_squared_error(y_25, y_pred))\n",
    "results_df = pd.DataFrame({'actual': y_25, 'predicted': y_pred})\n",
    "results_df = results_df.join(df[['batter', 'year']])\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e96e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb.plot_importance(models[.9], importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9753356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for batters with more than 60 plate appearances: 0.0468476478233799\n",
      "Grouped RMSE 0.11272411592017098\n"
     ]
    }
   ],
   "source": [
    "grouped_results = results_df.groupby(['year', 'batter'])[['actual', 'predicted']].agg(['mean', 'count'])\n",
    "grouped_results.columns = ['_'.join(col).strip() for col in grouped_results.columns.values]\n",
    "grouped_results = grouped_results.reset_index()\n",
    "grouped_rmse = np.sqrt(mean_squared_error(grouped_results['actual_mean'], grouped_results['predicted_mean']))\n",
    "qualified_results = grouped_results[grouped_results['actual_count'] > 60]\n",
    "qualified_rmse = np.sqrt(mean_squared_error(qualified_results['actual_mean'], qualified_results['predicted_mean']))\n",
    "print(f'RMSE for batters with more than 60 plate appearances: {qualified_rmse}')\n",
    "print(f'Grouped RMSE {grouped_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4f868",
   "metadata": {},
   "source": [
    "##### Find Quintiles for Each Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4859dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m quantile_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m:\n\u001b[0;32m      4\u001b[0m     quantile_predictions[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m models[q]\u001b[38;5;241m.\u001b[39mpredict(x_25)\n\u001b[0;32m      6\u001b[0m quantile_predictions\u001b[38;5;241m.\u001b[39mset_index(x_25\u001b[38;5;241m.\u001b[39mindex, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "quantile_predictions = pd.DataFrame()\n",
    "\n",
    "for q in models:\n",
    "    quantile_predictions[f'q_{q}'] = models[q].predict(x_25)\n",
    "\n",
    "quantile_predictions.set_index(x_25.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdeccc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_predictions['name'] = df_25['batter']\n",
    "quantile_predictions['year'] = df_25['year']\n",
    "quantile_cols = sorted([col for col in quantile_predictions.columns if col.startswith('q_')])\n",
    "quantile_predictions[quantile_cols] = np.sort(quantile_predictions[quantile_cols].values, axis=1)\n",
    "quantile_predictions[quantile_cols] = quantile_predictions[quantile_cols].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80a8063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 players with highest prediction standard deviation:\n",
      "name\n",
      "aaron judge      0.012219\n",
      "adael amador     0.012219\n",
      "kyren paris      0.011666\n",
      "shohei ohtani    0.011569\n",
      "luis torrens     0.011422\n",
      "Name: pred_std, dtype: float64\n",
      "Top 5 players with lowest prediction standard deviation:\n",
      "name\n",
      "jonah bride       0.006798\n",
      "tim anderson      0.006936\n",
      "brooks baldwin    0.007335\n",
      "gary sanchez      0.007544\n",
      "trey sweeney      0.007628\n",
      "Name: pred_std, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "player_quant = quantile_predictions.groupby('name').mean()\n",
    "player_quant['pitch_count'] = quantile_predictions.groupby('name').size()\n",
    "player_quant['pred_std'] = player_quant[quantile_cols].std(axis=1)\n",
    "player_quant = player_quant[player_quant['pitch_count'] > 60]\n",
    "\n",
    "\n",
    "# Find and print the top 5 players with the highest standard deviation\n",
    "print(\"Top 5 players with highest prediction standard deviation:\")\n",
    "top_5_highest_std = player_quant.nlargest(5, 'pred_std')\n",
    "print(top_5_highest_std['pred_std'])\n",
    "\n",
    "# Find and print the top 5 players with the lowest standard deviation\n",
    "print(\"Top 5 players with lowest prediction standard deviation:\")\n",
    "top_5_lowest_std = player_quant.nsmallest(5, 'pred_std')\n",
    "print(top_5_lowest_std['pred_std'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e20053",
   "metadata": {},
   "source": [
    "##### Quant Predections For Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920d4f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 661\n",
      "[LightGBM] [Info] Number of data points in the train set: 866880, number of used features: 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Info] Start training from score 0.384663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for q in quantiles:\n",
    "    quantile_model = lgb.LGBMRegressor(**ev_dir_params[str(q)], alpha=q, random_state=26, n_jobs=-1)\n",
    "    quantile_model.fit(X, y, \n",
    "                       eval_set=[(x_val, y_val)], \n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=40, verbose=False)])\n",
    "    models[q] = quantile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9413a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=25, reg_lambda=0.0 will be ignored. Current value: lambda_l2=25\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=9, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=9\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=20, reg_lambda=0.0 will be ignored. Current value: lambda_l2=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] lambda_l1 is set=3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=50, reg_lambda=0.0 will be ignored. Current value: lambda_l2=50\n"
     ]
    }
   ],
   "source": [
    "full_predictions = pd.DataFrame()\n",
    "\n",
    "for q in models:\n",
    "    full_predictions[f'q_{q}'] = models[q].predict(X)\n",
    "\n",
    "full_predictions.set_index(X.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d419b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predictions['name'] = df['batter']\n",
    "full_predictions['year'] = df['year']\n",
    "quantile_cols = sorted([col for col in full_predictions.columns if col.startswith('q_')])\n",
    "full_predictions[quantile_cols] = np.sort(full_predictions[quantile_cols].values, axis=1)\n",
    "full_predictions[quantile_cols] = full_predictions[quantile_cols].clip(lower=0)\n",
    "full_predictions[quantile_cols] = full_predictions[quantile_cols].clip(upper=2.01775) # average hr woba over last 8 years\n",
    "full_predictions = full_predictions.reset_index()\n",
    "cols = ['name', 'year'] + [col for col in full_predictions.columns if col not in ['name', 'year', 'index']]\n",
    "full_predictions = full_predictions[cols]\n",
    "full_predictions.to_csv('quantile_predections/ev_dir_pitch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30b92e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_predictions = full_predictions.groupby(['name', 'year']).mean()\n",
    "player_predictions['pitch_count'] = full_predictions.groupby(['name', 'year']).size()\n",
    "player_predictions = player_predictions.reset_index()\n",
    "cols = list(player_predictions.columns)\n",
    "cols.remove('index')\n",
    "cols.insert(2, 'pitch_count')\n",
    "player_predictions = player_predictions[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b82634",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_predictions.to_csv('predections/ev_direction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
